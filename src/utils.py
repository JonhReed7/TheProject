"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏.

–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
- –û—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –†–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏
"""

import re
import os
from typing import List, Dict, Any, Optional
from pathlib import Path


def tokenize_sentences(text: str) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    
    –£—á–∏—Ç—ã–≤–∞–µ—Ç:
    - –¢–æ—á–∫–∏, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏
    - –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ
    - –°–æ–∫—Ä–∞—â–µ–Ω–∏—è (Mr., Dr., etc.)
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        
    Examples:
        >>> tokenize_sentences("Hello world. How are you?")
        ['Hello world', 'How are you']
    """
    if not text:
        return []
    
    # –ó–∞—â–∏—Ç–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
    abbreviations = ['Mr.', 'Mrs.', 'Dr.', 'Prof.', 'Jr.', 'Sr.', 
                     'vs.', 'etc.', 'e.g.', 'i.e.', '—Ç.–¥.', '—Ç.–ø.', 
                     '–¥—Ä.', '–ø—Ä.', '–≥.', '–≥–≥.']
    
    protected_text = text
    for abbr in abbreviations:
        protected_text = protected_text.replace(abbr, abbr.replace('.', '<DOT>'))
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∫–æ–Ω—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = re.split(r'[.!?]+', protected_text)
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –∏ –æ—á–∏—â–∞–µ–º
    result = []
    for s in sentences:
        s = s.replace('<DOT>', '.').strip()
        if s:
            result.append(s)
    
    return result


def tokenize_words(text: str) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞.
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è:
    - –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    - –ß–∏—Å–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        
    Examples:
        >>> tokenize_words("Hello, World! 123")
        ['hello', 'world']
    """
    if not text:
        return []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å]+\b', text.lower())
    
    return words


def count_characters(words: List[str]) -> int:
    """
    –ü–æ–¥—Å—á—ë—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ö.
    
    Args:
        words: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤
        
    Returns:
        –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É–∫–≤
    """
    return sum(len(word) for word in words)


def clean_text(text: str) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text:
        return ""
    
    # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    text = text.strip()
    
    return text


def detect_language(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—Ä—É—Å—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π).
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        'ru' –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ, 'en' –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
    """
    if not text:
        return 'en'
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –±—É–∫–≤—ã
    russian_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    return 'ru' if russian_chars > english_chars else 'en'


def format_result_as_dict(result: Any) -> Dict[str, Any]:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Å–ª–æ–≤–∞—Ä—å.
    
    Args:
        result: –û–±—ä–µ–∫—Ç ReadabilityResult
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    return {
        'text_length': result.text_length,
        'word_count': result.word_count,
        'sentence_count': result.sentence_count,
        'avg_word_length': result.avg_word_length,
        'avg_sentence_length': result.avg_sentence_length,
        'metrics': {
            'flesch_reading_ease': result.flesch_score,
            'coleman_liau_index': result.coleman_liau,
            'automated_readability_index': result.ari,
        },
        'difficulty_level': result.difficulty_level,
        'target_audience': result.target_audience,
        'recommendations': result.recommendations,
    }


def format_result_as_markdown(result: Any, title: str = "Analysis Result") -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ Markdown.
    
    Args:
        result: –û–±—ä–µ–∫—Ç ReadabilityResult
        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞
        
    Returns:
        –°—Ç—Ä–æ–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    """
    lines = [
        f"## üìä {title}",
        "",
        "### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏",
        "",
        "| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |",
        "|---------|----------|",
        f"| –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ | {result.text_length} —Å–∏–º–≤–æ–ª–æ–≤ |",
        f"| –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ | {result.word_count} |",
        f"| –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π | {result.sentence_count} |",
        f"| –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ | {result.avg_word_length} –±—É–∫–≤ |",
        f"| –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è | {result.avg_sentence_length} —Å–ª–æ–≤ |",
        "",
        "### –ò–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏",
        "",
        "| –ò–Ω–¥–µ–∫—Å | –ó–Ω–∞—á–µ–Ω–∏–µ |",
        "|--------|----------|",
        f"| Flesch Reading Ease | {result.flesch_score} |",
        f"| Coleman-Liau Index | {result.coleman_liau} |",
        f"| ARI | {result.ari} |",
        "",
        "### –ó–∞–∫–ª—é—á–µ–Ω–∏–µ",
        "",
        f"**–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:** {result.difficulty_level}",
        "",
        f"**–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {result.target_audience}",
        "",
        "### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
        "",
    ]
    
    for rec in result.recommendations:
        lines.append(f"- {rec}")
    
    return "\n".join(lines)


def read_text_file(filepath: str, encoding: str = 'utf-8') -> str:
    """
    –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞.
    
    Args:
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞
        
    Returns:
        –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        
    Raises:
        FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
        IOError: –ü—Ä–∏ –æ—à–∏–±–∫–µ —á—Ç–µ–Ω–∏—è
    """
    path = Path(filepath)
    
    if not path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
    
    with open(path, 'r', encoding=encoding) as f:
        return f.read()


def save_report(content: str, filepath: str, encoding: str = 'utf-8') -> None:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ —Ñ–∞–π–ª.
    
    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç—á—ë—Ç–∞
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞
    """
    path = Path(filepath)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(path, 'w', encoding=encoding) as f:
        f.write(content)


def get_text_statistics(text: str) -> Dict[str, int]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç—É.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    words = tokenize_words(text)
    sentences = tokenize_sentences(text)
    
    return {
        'characters_total': len(text),
        'characters_no_spaces': len(text.replace(' ', '')),
        'words': len(words),
        'sentences': len(sentences),
        'paragraphs': text.count('\n\n') + 1,
        'unique_words': len(set(words)),
    }