#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/generate_report.py --output reports/report.md --language en
    
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ data/sample_texts/ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç
Markdown-–æ—Ç—á—ë—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
"""

import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.analyzer import TextAnalyzer


def parse_arguments():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"
    )
    parser.add_argument(
        '--output', '-o',
        default='reports/analysis_report.md',
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É (default: reports/analysis_report.md)'
    )
    parser.add_argument(
        '--language', '-l',
        default='auto',
        choices=['en', 'ru', 'auto'],
        help='–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–æ–≤ (default: auto)'
    )
    parser.add_argument(
        '--input-dir', '-i',
        default='data/sample_texts',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ–∫—Å—Ç–∞–º–∏ (default: data/sample_texts)'
    )
    return parser.parse_args()


def generate_report(analyzer: TextAnalyzer, 
                    input_dir: Path, 
                    language: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Args:
        analyzer: –≠–∫–∑–µ–º–ø–ª—è—Ä TextAnalyzer
        input_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        language: –Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å –æ—Ç—á—ë—Ç–æ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    """
    lines = [
        "# üìä Readability Analysis Report",
        "",
        f"**–î–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        f"**–Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞:** {language}",
        "",
        f"**–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è:** `{input_dir}`",
        "",
        "---",
        "",
        "## üìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞",
        "",
        "| –§–∞–π–ª | –°–ª–æ–≤ | –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π | Flesch | –°–ª–æ–∂–Ω–æ—Å—Ç—å |",
        "|------|------|-------------|--------|-----------|",
    ]
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    detailed_results = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ .txt —Ñ–∞–π–ª—ã
    txt_files = sorted(input_dir.glob("*.txt"))
    
    if not txt_files:
        lines.append("| - | –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ | - | - | - |")
    
    for txt_file in txt_files:
        try:
            result = analyzer.analyze_file(str(txt_file))
            
            # –°—Ç—Ä–æ–∫–∞ –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
            lines.append(
                f"| {txt_file.name} | {result.word_count} | "
                f"{result.sentence_count} | {result.flesch_score} | "
                f"{result.difficulty_level} |"
            )
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            detailed_results.append((txt_file.name, result))
            
        except Exception as e:
            lines.append(f"| {txt_file.name} | ‚ùå –û—à–∏–±–∫–∞ | - | - | {str(e)[:30]}... |")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    lines.extend([
        "",
        "---",
        "",
        "## üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
        "",
    ])
    
    for filename, result in detailed_results:
        lines.extend([
            f"### üìù {filename}",
            "",
            "#### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏",
            "",
            "| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |",
            "|---------|----------|",
            f"| –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ | {result.text_length} —Å–∏–º–≤–æ–ª–æ–≤ |",
            f"| –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ | {result.word_count} |",
            f"| –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π | {result.sentence_count} |",
            f"| –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ | {result.avg_word_length} –±—É–∫–≤ |",
            f"| –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è | {result.avg_sentence_length} —Å–ª–æ–≤ |",
            "",
            "#### –ò–Ω–¥–µ–∫—Å—ã —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏",
            "",
            "| –ò–Ω–¥–µ–∫—Å | –ó–Ω–∞—á–µ–Ω–∏–µ |",
            "|--------|----------|",
            f"| Flesch Reading Ease | {result.flesch_score} |",
            f"| Flesch-Kincaid Grade | {result.flesch_kincaid} |",
            f"| Coleman-Liau Index | {result.coleman_liau} |",
            f"| Automated Readability Index | {result.ari} |",
            "",
            "#### –ó–∞–∫–ª—é—á–µ–Ω–∏–µ",
            "",
            f"- **–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:** {result.difficulty_level}",
            f"- **–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {result.target_audience}",
            "",
            "#### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
            "",
        ])
        
        for rec in result.recommendations:
            lines.append(f"- {rec}")
        
        lines.extend(["", "---", ""])
    
    # –ü–æ–¥–≤–∞–ª –æ—Ç—á—ë—Ç–∞
    lines.extend([
        "",
        "## ‚ÑπÔ∏è –û –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏",
        "",
        "–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤:",
        "",
        "- **Flesch Reading Ease** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ (0-100)",
        "- **Flesch-Kincaid Grade** ‚Äî —É—Ä–æ–≤–µ–Ω—å –∫–ª–∞—Å—Å–∞ –ø–æ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ",
        "- **Coleman-Liau Index** ‚Äî –∏–Ω–¥–µ–∫—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
        "- **ARI** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏",
        "",
        "–ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ñ–æ—Ä–º—É–ª–∞—Ö: [docs/formulas.md](../docs/formulas.md)",
        "",
        "---",
        "",
        f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now().isoformat()}*",
    ])
    
    return "\n".join(lines)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    args = parse_arguments()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = TextAnalyzer(language=args.language)
    
    # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
    input_dir = project_root / args.input_dir
    
    if not input_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
        sys.exit(1)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    print(f"üìÇ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –≤: {input_dir}")
    report = generate_report(analyzer, input_dir, args.language)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    output_path = project_root / args.output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    
    # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    txt_files = list(input_dir.glob("*.txt"))
    print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(txt_files)}")


if __name__ == "__main__":
    main()